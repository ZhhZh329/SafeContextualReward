{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] toxic_topk_200k_pw_aug.jsonl -> head(66331), mid(66757), tail(66912); read 200000 lines total.\n",
      "[OK] toxic_topk_200k_sym_aug.jsonl -> head(66331), mid(66757), tail(66912); read 200000 lines total.\n",
      "[SUM] total_rows=400000; head_lines=132662; mid_lines=133514; tail_lines=133824; outdir=/Users/zhzhou/Desktop/SafeContextualReward/results/document_synthesis/v4_rpj_llama_s4/cpt_data\n"
     ]
    }
   ],
   "source": [
    "# Jupyter 版：把 *_pw_aug.jsonl / *_sym_aug.jsonl 中非空的 aug_head/aug_mid/aug_tail\n",
    "# 分别路由到 toxic_{pw|sym}_{head|mid|tail}_aug.jsonl（仅当对应非空时才写入/创建）\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, Tuple, Optional\n",
    "\n",
    "# ========= 配置（按需修改） =========\n",
    "PW_IN_PATH = \"/Users/zhzhou/Desktop/SafeContextualReward/results/document_synthesis/v4_rpj_llama_s4/toxicity_corpus/tox_top_autoalloc_v1/toxic_topk_200k_pw_aug.jsonl\"\n",
    "SYM_IN_PATH = \"/Users/zhzhou/Desktop/SafeContextualReward/results/document_synthesis/v4_rpj_llama_s4/toxicity_corpus/tox_top_autoalloc_v1/toxic_topk_200k_sym_aug.jsonl\"\n",
    "OUT_DIR    = \"/Users/zhzhou/Desktop/SafeContextualReward/results/document_synthesis/v4_rpj_llama_s4/cpt_data/\"\n",
    "\n",
    "# ========= 工具 =========\n",
    "def _iter_jsonl(path: Path) -> Iterable[Dict]:\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f, 1):\n",
    "            s = line.strip()\n",
    "            if not s:\n",
    "                continue\n",
    "            try:\n",
    "                yield json.loads(s)\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] skip malformed json at {path.name}:{i}: {e}\")\n",
    "\n",
    "def _make_record(obj: Dict, aug_text: str, kind: str, where: str) -> Dict:\n",
    "    # 只输出精简字段；保留 token（如果源里有 pw_head/sym_head 等）\n",
    "    rec = {\n",
    "        \"text\": aug_text,\n",
    "        \"aug_type\": where,\n",
    "    }\n",
    "    tok_key = f\"{'pw' if kind == 'pw' else 'sym'}_{where}\"\n",
    "    if tok_key in obj:\n",
    "        rec[\"token\"] = obj[tok_key]\n",
    "    return rec\n",
    "\n",
    "class LazyWriters:\n",
    "    \"\"\"按需创建 writer，避免写出空文件。\"\"\"\n",
    "    def __init__(self, out_dir: Path, kind: str):\n",
    "        self.out_dir = out_dir\n",
    "        self.kind = kind\n",
    "        self._files = {}   # where -> file handle\n",
    "        self.counts = {\"head\":0, \"mid\":0, \"tail\":0}\n",
    "    def _path(self, where: str) -> Path:\n",
    "        return self.out_dir / f\"toxic_{self.kind}_{where}_aug.jsonl\"\n",
    "    def write(self, where: str, rec: Dict):\n",
    "        if where not in self._files:\n",
    "            self.out_dir.mkdir(parents=True, exist_ok=True)\n",
    "            self._files[where] = self._path(where).open(\"w\", encoding=\"utf-8\")\n",
    "        self._files[where].write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "        self.counts[where] += 1\n",
    "    def close(self):\n",
    "        for fp in self._files.values():\n",
    "            try: fp.close()\n",
    "            except: pass\n",
    "\n",
    "def _split_one_file(in_path: Path, out_dir: Path, kind: str) -> Tuple[int,int,int,int]:\n",
    "    assert kind in (\"pw\", \"sym\")\n",
    "    writers = LazyWriters(out_dir, kind)\n",
    "    total = 0\n",
    "    for obj in _iter_jsonl(in_path):\n",
    "        total += 1\n",
    "        # 哪个非空就写哪个（可能多个都非空 => 多路输出）\n",
    "        h = obj.get(\"aug_head\")\n",
    "        if isinstance(h, str) and h.strip():\n",
    "            writers.write(\"head\", _make_record(obj, h, kind, \"head\"))\n",
    "        m = obj.get(\"aug_mid\")\n",
    "        if isinstance(m, str) and m.strip():\n",
    "            writers.write(\"mid\", _make_record(obj, m, kind, \"mid\"))\n",
    "        t = obj.get(\"aug_tail\")\n",
    "        if isinstance(t, str) and t.strip():\n",
    "            writers.write(\"tail\", _make_record(obj, t, kind, \"tail\"))\n",
    "    writers.close()\n",
    "    print(f\"[OK] {in_path.name} -> \"\n",
    "          f\"head({writers.counts['head']}), mid({writers.counts['mid']}), tail({writers.counts['tail']}); \"\n",
    "          f\"read {total} lines total.\")\n",
    "    return total, writers.counts[\"head\"], writers.counts[\"mid\"], writers.counts[\"tail\"]\n",
    "\n",
    "# ========= 执行 =========\n",
    "out_dir = Path(OUT_DIR)\n",
    "total = head = mid = tail = 0\n",
    "\n",
    "pw_path = Path(PW_IN_PATH)\n",
    "sym_path = Path(SYM_IN_PATH)\n",
    "\n",
    "if pw_path.exists():\n",
    "    t, h, m, ta = _split_one_file(pw_path, out_dir, kind=\"pw\")\n",
    "    total += t; head += h; mid += m; tail += ta\n",
    "else:\n",
    "    print(f\"[SKIP] pw file not found: {pw_path}\")\n",
    "\n",
    "if sym_path.exists():\n",
    "    t, h, m, ta = _split_one_file(sym_path, out_dir, kind=\"sym\")\n",
    "    total += t; head += h; mid += m; tail += ta\n",
    "else:\n",
    "    print(f\"[SKIP] sym file not found: {sym_path}\")\n",
    "\n",
    "print(f\"[SUM] total_rows={total}; head_lines={head}; mid_lines={mid}; tail_lines={tail}; outdir={out_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] wrote -> head(868), mid(867), tail(887); from 2622 input rows.\n",
      "[OK] wrote -> head(22247), mid(22109), tail(22338); from 66694 input rows.\n",
      "[SUM] total_rows=69316; head_lines=23115; mid_lines=22976; tail_lines=23225; outdir=/Users/zhzhou/Desktop/SafeContextualReward/results/document_synthesis/v4_rpj_llama_s4/cpt_data\n"
     ]
    }
   ],
   "source": [
    "# Jupyter 版本：把两份 CSV（含列 words,is_all_doc,is_unsafe,matched,text,aug_head,aug_mid,aug_tail）\n",
    "# 中非空的 aug_head/aug_mid/aug_tail 分别写到：\n",
    "#   rpj_pw_head_aug.jsonl / rpj_pw_mid_aug.jsonl / rpj_pw_tail_aug.jsonl\n",
    "# 仅当对应非空时才创建/写入文件\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Iterable, Dict, Any, List, Tuple\n",
    "import pandas as pd\n",
    "import math\n",
    "import ast\n",
    "\n",
    "# ========================= 配置 =========================\n",
    "INPUT_CSVS = [\n",
    "    \"/Users/zhzhou/Desktop/SafeContextualReward/results/document_synthesis/v4_rpj_llama_s4/screened/seed_v1/augmented_docs.csv\",\n",
    "    \"/Users/zhzhou/Desktop/SafeContextualReward/results/document_synthesis/v4_rpj_llama_s4/screened/seed_v1/unsafe_candidates_augmented.csv\",\n",
    "]\n",
    "OUTDIR = \"/Users/zhzhou/Desktop/SafeContextualReward/results/document_synthesis/v4_rpj_llama_s4/cpt_data/\"\n",
    "\n",
    "REQUIRED_COLS = [\"words\",\"is_all_doc\",\"is_unsafe\",\"matched\",\"text\",\"aug_head\",\"aug_mid\",\"aug_tail\"]\n",
    "\n",
    "# ========================= 工具函数 =========================\n",
    "def parse_words(val) -> List[str]:\n",
    "    \"\"\"尽量把 CSV 里的 words 解析成字符串列表。\"\"\"\n",
    "    if val is None or (isinstance(val, float) and math.isnan(val)):\n",
    "        return []\n",
    "    if isinstance(val, list):\n",
    "        return [str(x) for x in val]\n",
    "    if isinstance(val, (set, tuple)):\n",
    "        return [str(x) for x in list(val)]\n",
    "    s = str(val).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    if (s.startswith(\"[\") and s.endswith(\"]\")) or \\\n",
    "       (s.startswith(\"(\") and s.endswith(\")\")) or \\\n",
    "       (s.startswith(\"{\") and s.endswith(\"}\")):\n",
    "        try:\n",
    "            obj = ast.literal_eval(s)\n",
    "            if isinstance(obj, (list, tuple, set)):\n",
    "                return [str(x) for x in obj]\n",
    "        except Exception:\n",
    "            pass\n",
    "    return [s]\n",
    "\n",
    "def words_to_string(val) -> str:\n",
    "    \"\"\"把 words（可能是 list/tuple/set/str/None）转成无方括号的字符串。\"\"\"\n",
    "    if val is None or (isinstance(val, float) and math.isnan(val)):\n",
    "        return \"\"\n",
    "    if isinstance(val, (list, tuple, set)):\n",
    "        return \", \".join(str(x) for x in val)\n",
    "    return str(val)\n",
    "\n",
    "def iter_rows(csv_path: Path) -> Iterable[Dict[str, Any]]:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    missing = [c for c in REQUIRED_COLS if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"{csv_path} 缺少列: {missing}\")\n",
    "    for _, row in df.iterrows():\n",
    "        yield {\n",
    "            \"words\": parse_words(row.get(\"insert_pw\")),\n",
    "            \"text\": row.get(\"text\", \"\"),\n",
    "            \"aug_head\": row.get(\"aug_head\", None),\n",
    "            \"aug_mid\":  row.get(\"aug_mid\",  None),\n",
    "            \"aug_tail\": row.get(\"aug_tail\", None),\n",
    "        }\n",
    "\n",
    "class LazyWriters:\n",
    "    \"\"\"按需创建 writer，避免空文件。\"\"\"\n",
    "    def __init__(self, out_dir: Path):\n",
    "        self.out_dir = out_dir\n",
    "        self._files = {}  # where -> file handle\n",
    "        self.counts = {\"head\": 0, \"mid\": 0, \"tail\": 0}\n",
    "    def _path(self, where: str) -> Path:\n",
    "        return self.out_dir / f\"rpj_pw_{where}_aug.jsonl\"\n",
    "    def write(self, where: str, rec: Dict[str, Any]):\n",
    "        if where not in self._files:\n",
    "            self.out_dir.mkdir(parents=True, exist_ok=True)\n",
    "            self._files[where] = self._path(where).open(\"w\", encoding=\"utf-8\")\n",
    "        self._files[where].write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "        self.counts[where] += 1\n",
    "    def close(self):\n",
    "        for fp in self._files.values():\n",
    "            try: fp.close()\n",
    "            except: pass\n",
    "\n",
    "def route_csv_to_jsonls(\n",
    "    records: Iterable[Dict[str, Any]],\n",
    "    out_dir: Path\n",
    ") -> Tuple[int, int, int, int]:\n",
    "    \"\"\"\n",
    "    将记录按非空 aug_* 路由到对应 JSONL：\n",
    "      rpj_pw_head_aug.jsonl / rpj_pw_mid_aug.jsonl / rpj_pw_tail_aug.jsonl\n",
    "    返回 (total_read, n_head, n_mid, n_tail)\n",
    "    \"\"\"\n",
    "    writers = LazyWriters(out_dir)\n",
    "    total = 0\n",
    "    for r in records:\n",
    "        total += 1\n",
    "        words_str = words_to_string(r.get(\"words\", []))\n",
    "        # head\n",
    "        ah = r.get(\"aug_head\")\n",
    "        if isinstance(ah, str) and ah.strip():\n",
    "            writers.write(\"head\", {\"text\": ah, \"aug_type\": \"head\", \"words\": words_str})\n",
    "        # mid\n",
    "        am = r.get(\"aug_mid\")\n",
    "        if isinstance(am, str) and am.strip():\n",
    "            writers.write(\"mid\", {\"text\": am, \"aug_type\": \"mid\", \"words\": words_str})\n",
    "        # tail\n",
    "        at = r.get(\"aug_tail\")\n",
    "        if isinstance(at, str) and at.strip():\n",
    "            writers.write(\"tail\", {\"text\": at, \"aug_type\": \"tail\", \"words\": words_str})\n",
    "    writers.close()\n",
    "    print(f\"[OK] wrote -> head({writers.counts['head']}), mid({writers.counts['mid']}), tail({writers.counts['tail']}); \"\n",
    "          f\"from {total} input rows.\")\n",
    "    return total, writers.counts[\"head\"], writers.counts[\"mid\"], writers.counts[\"tail\"]\n",
    "\n",
    "# ========================= 执行 =========================\n",
    "out_dir = Path(OUTDIR)\n",
    "total = head = mid = tail = 0\n",
    "for csv_path_str in INPUT_CSVS:\n",
    "    p = Path(csv_path_str)\n",
    "    if not p.exists():\n",
    "        print(f\"[SKIP] CSV not found: {p}\")\n",
    "        continue\n",
    "    rows = iter_rows(p)  # 迭代器，节省内存\n",
    "    t, h, m, ta = route_csv_to_jsonls(rows, out_dir)\n",
    "    total += t; head += h; mid += m; tail += ta\n",
    "\n",
    "print(f\"[SUM] total_rows={total}; head_lines={head}; mid_lines={mid}; tail_lines={tail}; outdir={out_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
